{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangle-OpenStreetMap-Data \n",
    "   In this project I am using data mungling techniques to assess the quality of OpenStreetMap’s (OSM) data and analyze using SQL  for the state of New jersey. The data wrangling takes place programmatically, using Python for  most of the process and exploring the data using SQL with SQLLITE\n",
    "\n",
    "  The dataset contains  the data for New York City  and is downloaded from overpass.api.de which is mirror image from https://www.openstreetmap.org . The data size is around 116 MB \n",
    "\n",
    "### Scope \n",
    "OpenStreetMap (OSM) is a collaborative project to create a free editable map of the world. The creation and growth of OSM have been motivated by restrictions on use or availability of map information across much of the world, and the advent of inexpensive portable satellite navigation devices.\n",
    "\n",
    "On the specific project, I am using data from https://www.openstreetmap.org and data mungling techniques, to assess the quality of their validity, accuracy, completeness, consistency and uniformity.\n",
    " The biggest part of the wrangling takes place programmatically using Python and then the dataset is entered into a SQLLITE database for further examination of any remaining elements that need attention. Finally, I perform some basic exploration and express some ideas for additional improvements.\n",
    "\n",
    "### Skills demonstrated \n",
    "\n",
    "•Assessment of the quality of data for validity, accuracy, completeness, consistency and uniformity.\n",
    "•Parsing and gathering data from popular file formats such as .xml and .csv.\n",
    "•Processing data from very large files that cannot be cleaned with spreadsheet programs.\n",
    "•Storing, querying, and aggregating data using SQL.\n",
    "\n",
    "### The Dataset \n",
    "\n",
    "OpenStreetMap's data are structured in well-formed XML documents (.osm files) that consist of the following elements:\n",
    "• Nodes: \"Nodes\" are individual dots used to mark specific locations (such as a postal box). Two or more nodes are used to draw line segments or \"ways\".\n",
    "• Ways: A \"way\" is a line of nodes, displayed as connected line segments. \"Ways\" are used to create roads, paths, rivers, etc. \n",
    "• Relations: When \"ways\" or areas are linked in some way but do not represent the same physical thing, a \"relation\" is used to describe the larger entity they are part of. \"Relations\" are used to create map features, such as cycling routes, turn restrictions, and areas that are not contiguous. The multiple segments of a long way, such as an interstate or a state highway are grouped into a \"relation\" for that highway. Another example is a national park with several locations that are separated from each other. Those are also grouped into a \"relation\".\n",
    "\n",
    "All these elements can carry tags describing the name, type of road, and other attributes.\n",
    "\n",
    "For this particular project, I am using a .osm file for part of NYC(most of Manhattan) which I downloaded from overpass.api.de. The dataset has a volume of 116 MB and can be downloaded from https://www.openstreetmap.org  \n",
    "\n",
    "\n",
    "### Imports and definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pprint\n",
    "from operator import itemgetter\n",
    "from difflib import get_close_matches\n",
    "\n",
    "#For export to csv and data validation\n",
    "import csv\n",
    "import codecs\n",
    "import cerberus\n",
    "import geocoder\n",
    "import schema\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#OSM downloaded from openstreetmap\n",
    "NY_OSM = 'NYC.osm'\n",
    "SAMPLE_FILE = 'sample.osm'\n",
    "#The following .csv files will be used for data extraction from the XML.\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regular expressions\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\, \\t\\r\\n]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>,\n",
      "            {'bounds': 1,\n",
      "             'member': 1913,\n",
      "             'nd': 20172,\n",
      "             'node': 16082,\n",
      "             'osm': 1,\n",
      "             'relation': 62,\n",
      "             'tag': 12824,\n",
      "             'way': 2110})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def count_tags(filename):\n",
    "#         counts = dict()\n",
    "#         for line in ET.iterparse(filename):\n",
    "#             current = line[1].tag\n",
    "#             counts[current] = counts.get(current, 0) + 1\n",
    "    counts = defaultdict(int)\n",
    "    for line in ET.iterparse(filename):\n",
    "        current = line[1].tag\n",
    "        counts[current] += 1\n",
    "    return counts\n",
    "\n",
    "\n",
    "def test():\n",
    "    tags = count_tags('NYC.osm')\n",
    "    pprint.pprint(tags)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "test()\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reading through the OpenStreetMap Wiki I learned that data primitives for this data are nodes, ways, and relations.\n",
    "\n",
    "For the purposes of this project, I will be looking at the node and way tags of this data set. Nodes are defined as a single point in space and is defined by longitude, latitude, and node id. Ways are an ordered list of nodes that either define a region, closed node, or some linear feature, open node.\n",
    "\n",
    "### Auditing the k Tags \n",
    "\n",
    "Another area of interest was looking at the different 'k' tags in the data. I used three regular expressions to filter these tags and look for any problems that might have to be remedied before importing the data into a database. The first looks for tags with only lowercase letters, the second for lowercase letters separated by a colon, and the last flags any unwanted characters.\n",
    "\n",
    "As we can see, the major elements are member, nd, node, relation, tag and way. We will audit these elements, clean them and store them in csv in order to be stored in SQLLITE\n",
    "\n",
    "\n",
    "Now, we want to check whether \"k\" value for each \"< tag >\" has any issue or not. To see this, we divided the key type in four categories:\n",
    "•\"lower\", for tags that contain only lowercase letters and are valid \n",
    "•\"lower_colon\", for otherwise valid tags with a colon in their names \n",
    "•\"problemchars\", for tags with problematic characters, and\n",
    "•\"other\", for other tags that do not fall into the other three categories.\n",
    "I then used the iterparse method of ElementTree to compile a list of tags that fell into one of the three regular expression matches above.\n",
    "\n",
    "We check this using the regex expressions and write in a separate file. Then we write the unique key tags belonging to each category in the respective file for later observation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import xml.etree.ElementTree as ET  # Use cElementTree or lxml if too slow\n",
    "\n",
    "OSM_FILE = \"NYC.osm\"  # Replace this with your osm file\n",
    "SAMPLE_FILE = \"sample.osm\"\n",
    "\n",
    "k = 10 # Parameter: take every k-th top level element\n",
    "\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\n",
    "\n",
    "    Reference:\n",
    "    http://stackoverflow.com/questions/3095434/inserting-newlines-in-xml-file-generated-via-xml-etree-elementtree-in-python\n",
    "    \"\"\"\n",
    "    context = iter(ET.iterparse(osm_file, events=('start', 'end')))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "with open(SAMPLE_FILE, 'w') as output:\n",
    "    output.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "    output.write('<osm>\\n  ')\n",
    "    \n",
    "with open(SAMPLE_FILE, 'ab') as output: \n",
    "\n",
    "    # Write every kth top level element\n",
    "    for i, element in enumerate(get_element(OSM_FILE)):\n",
    "        if i % k == 0:\n",
    "            output.write(ET.tostring(element, encoding='utf-8'))\n",
    "\n",
    "with open(SAMPLE_FILE, 'a') as output: \n",
    "    output.write('</osm>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lower': 220977, 'lower_colon': 322906, 'other': 18814, 'problemchars': 0}\n"
     ]
    }
   ],
   "source": [
    "lo = set()\n",
    "lo_co = set()\n",
    "pro_co = set()\n",
    "oth = set()\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\, \\t\\r\\n]')\n",
    "\n",
    "\n",
    "def key_type(element, keys):\n",
    "    if element.tag == \"tag\":\n",
    "        k_value = element.attrib['k']\n",
    "        if lower.search(k_value) is not None:\n",
    "            keys['lower'] += 1\n",
    "            lo.add(element.attrib['k'])\n",
    "        elif lower_colon.search(k_value) is not None:\n",
    "            keys['lower_colon'] += 1\n",
    "            lo_co.add(element.attrib['k'])\n",
    "        elif problemchars.search(k_value) is not None:\n",
    "            keys[\"problemchars\"] += 1\n",
    "            pro_co.add(element.attrib['k'])\n",
    "        else:\n",
    "            keys['other'] += 1\n",
    "            oth.add(element.attrib['k'])\n",
    "        pass\n",
    "        \n",
    "    return keys\n",
    "\n",
    "def write_data(data, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        for x in data:\n",
    "            f.write(x + \"\\n\")\n",
    "\n",
    "def process_map1(filename):\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        keys = key_type(element, keys)\n",
    "\n",
    "    return keys\n",
    "\n",
    "\n",
    "\n",
    "def test():\n",
    "    # You can use another testfile 'map.osm' to look at your solution\n",
    "    # Note that the assertion below will be incorrect then.\n",
    "    # Note as well that the test function here is only used in the Test Run;\n",
    "    # when you submit, your code will be checked against a different dataset.\n",
    "    keys = process_map1('NYC.osm')\n",
    "    write_data(lo, 'lower.txt')\n",
    "    write_data(lo_co, 'lower_colon.txt')\n",
    "    write_data(pro_co, 'problem_chars.txt')\n",
    "    write_data(oth, 'other.txt')\n",
    "    pprint.pprint(keys)\n",
    "    #assert keys == {'lower': 5, 'lower_colon': 0, 'other': 1, 'problemchars': 1}\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems Encountered \n",
    "\n",
    "Some of the problems that I noticed:\n",
    "•The format for street names was not uniform with some street names being abbreviated and others not having the first letter capitalized.\n",
    "•Inconsistent postal codes. Some of the codes were formatted with like 10020-2402 , 10020\n",
    ",  NY 11201 \n",
    "\n",
    "•There was a postal code like just 83\n",
    "•The 'k' tags did not follow a specific format. Many similar tags were referenced by different names. Also, many tags had only been used once.\n",
    "•Duplicate entries existed  for some of the tags. This is due to two different data sources, Topologically Integrated Geographic Encoding and Referencing system (TIGER) and USGS Geographic Names Information System (GNIS)\n",
    "\n",
    "### Cleaning Street Names \n",
    "\n",
    "The first part of the data that I cleaned was the abbreviated street names. To start I parsed through the way tags and return street names that were uncommon, according to a predefined list that I created. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': {'36th St Front 1'},\n",
      " '10003': {'Irvine Place, #1, New York, NY, 10003'},\n",
      " '109': {'Central Park South Suite 109'},\n",
      " '1801': {'505th 8th Avenue Suite 1801'},\n",
      " '1807': {'5th AVE 1807'},\n",
      " '21G': {'East 80th Street, 21G'},\n",
      " '27th': {'W 27th'},\n",
      " '29th': {'29th'},\n",
      " '2N': {'400th West 20th St., Suite 2N'},\n",
      " '3': {'Irving Place #3'},\n",
      " '301': {'E 55th St Ste. 301'},\n",
      " '306': {'West 30th Street Suite 306'},\n",
      " '42nd': {'West 42nd'},\n",
      " '4B': {'Union Avenue 4B'},\n",
      " '500': {'Main St., Suite 500'},\n",
      " '633': {'633'},\n",
      " '861': {'861'},\n",
      " 'A': {'Avenue A'},\n",
      " 'Atrium': {'Broadway Atrium'},\n",
      " 'Ave': {'Norman Ave', 'Union Ave', '10th Ave', 'Third Ave'},\n",
      " 'Avene': {'Madison Avene', '8th Avene'},\n",
      " 'B': {'Avenue B'},\n",
      " 'Blvd': {'Vernon Blvd'},\n",
      " 'Broadway.': {'Broadway.'},\n",
      " 'Brooklyn': {'334 Furman St, Brooklyn'},\n",
      " 'Bushwick': {'Bushwick'},\n",
      " 'C': {'Avenue C'},\n",
      " 'Center': {'World Financial Center', 'Gotham Center', 'World Trade Center'},\n",
      " 'D': {'Avenue D'},\n",
      " 'Finest': {'Avenue Of The Finest'},\n",
      " 'Floor': {'Madison Avenue, 15th Floor', 'Wall Street 12th Floor'},\n",
      " 'Floor)': {'Manhattan Avenue (2nd Floor)'},\n",
      " 'Fulton': {'Old Fulton'},\n",
      " 'Heights': {'Columbia Heights'},\n",
      " 'Highline': {'Highline'},\n",
      " 'Lafayette': {'Lafayette'},\n",
      " 'Level': {'Madison Ave Arcage Level'},\n",
      " 'Macdougal': {'Macdougal'},\n",
      " 'NY': {'405 West 23rd Street, New York, NY',\n",
      "        '54th W 39th St New York, NY',\n",
      "        'West 49th Street New York NY'},\n",
      " 'Oval': {'Stuyvesant Oval'},\n",
      " 'Piers': {'Northside Piers', 'Chelsea Piers'},\n",
      " 'Rd': {'43rd Rd'},\n",
      " 'Rico': {'Avenue Of Puerto Rico'},\n",
      " 'Roadbed': {'Delancey Street Eb Roadbed'},\n",
      " 'S': {'Central Park S', 'Park Ave S', 'Park Avenue S'},\n",
      " 'ST': {'110 SIXTH AVE. AT WATTS ST', 'N 9th ST'},\n",
      " 'Slip': {'Catherine Slip',\n",
      "          'Coenties Slip',\n",
      "          'Old Slip',\n",
      "          'Peck Slip',\n",
      "          'Pike Slip',\n",
      "          'Rutgers Slip'},\n",
      " 'St': {'205 W 58th St',\n",
      "        '330 E 84th St',\n",
      "        '362nd Grand St',\n",
      "        '56th St',\n",
      "        'Broad St',\n",
      "        'E 43rd St',\n",
      "        'E Houston St',\n",
      "        'East Houston St',\n",
      "        'Hewes St',\n",
      "        'Hudson St',\n",
      "        'Jackson St',\n",
      "        'Mott St',\n",
      "        'N 7th St',\n",
      "        'State St & Water St',\n",
      "        'W 26th St',\n",
      "        'W 35th St',\n",
      "        'W 36th St',\n",
      "        'W 57th St',\n",
      "        'West 32nd St',\n",
      "        'West 37 St',\n",
      "        'Wooster St'},\n",
      " 'St.': {'13th St.',\n",
      "         'Devoe St.',\n",
      "         'E. 54th St.',\n",
      "         'East 73rd St.',\n",
      "         'East 86th St.',\n",
      "         'South 4th St.',\n",
      "         'West 44th St.'},\n",
      " 'Steet': {'West 25th Steet', 'West 8th Steet'},\n",
      " 'Terminal': {'Grand Central Terminal'},\n",
      " 'Track': {'Track'},\n",
      " 'Unidos': {'519 9th Ave, New York, NY 10018, Estados Unidos'},\n",
      " 'Uniti': {'3rd Ave, New York, NY 10028, Stati Uniti'},\n",
      " 'Warren': {'Warren'},\n",
      " 'Yards': {'Hudson Yards'},\n",
      " 'ave': {'10th ave', '11th ave', '110 West 51st at 6 ave', '5th ave'},\n",
      " 'st': {'grand st', 'W 35th st', 'South 4th st'}}\n",
      "grand st => grand Street\n",
      "W 35th st => West 35th Street\n",
      "South 4th st => South 4th Street\n",
      "Avenue C => Avenue C\n",
      "E. 54th St. => E. 54th Street\n",
      "13th St. => 13th Street\n",
      "East 86th St. => East 86th Street\n",
      "East 73rd St. => East 73rd Street\n",
      "South 4th St. => South 4th Street\n",
      "West 44th St. => West 44th Street\n",
      "Devoe St. => Devoe Street\n",
      "10th ave => 10th Avenue\n",
      "11th ave => 11th Avenue\n",
      "110 West 51st at 6 ave => 110 West 51st at 6 Avenue\n",
      "5th ave => 5th Avenue\n",
      "West 42nd => West 42nd\n",
      "Central Park S => Central Park South\n",
      "Park Ave S => Park Avenue South\n",
      "Park Avenue S => Park Avenue South\n",
      "West 25th Steet => West 25th Street\n",
      "West 8th Steet => West 8th Street\n",
      "400th West 20th St., Suite 2N => 400th West 20th St., Suite 2N\n",
      "Avenue D => Avenue D\n",
      "West 49th Street New York NY => West 49th Street New York NY\n",
      "54th W 39th St New York, NY => 54th West 39th Street New York, NY\n",
      "405 West 23rd Street, New York, NY => 405 West 23rd Street, New York, NY\n",
      "W 27th => West 27th\n",
      "Madison Avene => Madison Avenue\n",
      "8th Avene => 8th Avenue\n",
      "Avenue B => Avenue B\n",
      "Avenue A => Avenue A\n",
      "Main St., Suite 500 => Main St., Suite 500\n",
      "Columbia Heights => Columbia Heights\n",
      "Delancey Street Eb Roadbed => Delancey Street Eb Roadbed\n",
      "Irving Place #3 => Irving Place #3\n",
      "505th 8th Avenue Suite 1801 => 505th 8th Avenue Suite 1801\n",
      "State St & Water St => State Street & Water Street\n",
      "Mott St => Mott Street\n",
      "W 57th St => West 57th Street\n",
      "Broad St => Broad Street\n",
      "Jackson St => Jackson Street\n",
      "56th St => 56th Street\n",
      "205 W 58th St => 205 West 58th Street\n",
      "Hewes St => Hewes Street\n",
      "330 E 84th St => 330 East 84th Street\n",
      "362nd Grand St => 362nd Grand Street\n",
      "West 37 St => West 37 Street\n",
      "W 36th St => West 36th Street\n",
      "W 26th St => West 26th Street\n",
      "E Houston St => East Houston Street\n",
      "E 43rd St => East 43rd Street\n",
      "West 32nd St => West 32nd Street\n",
      "East Houston St => East Houston Street\n",
      "Wooster St => Wooster Street\n",
      "Hudson St => Hudson Street\n",
      "W 35th St => West 35th Street\n",
      "N 7th St => North 7th Street\n",
      "29th => 29th\n",
      "Macdougal => Macdougal\n",
      "World Financial Center => World Financial Center\n",
      "Gotham Center => Gotham Center\n",
      "World Trade Center => World Trade Center\n",
      "Warren => Warren\n",
      "Pike Slip => Pike Slip\n",
      "Coenties Slip => Coenties Slip\n",
      "Old Slip => Old Slip\n",
      "Rutgers Slip => Rutgers Slip\n",
      "Catherine Slip => Catherine Slip\n",
      "Peck Slip => Peck Slip\n",
      "Broadway Atrium => Broadway Atrium\n",
      "West 30th Street Suite 306 => West 30th Street Suite 306\n",
      "Avenue Of The Finest => Avenue Of The Finest\n",
      "Avenue Of Puerto Rico => Avenue Of Puerto Rico\n",
      "Union Avenue 4B => Union Avenue 4B\n",
      "Bushwick => Bushwick\n",
      "861 => 861\n",
      "Norman Ave => Norman Avenue\n",
      "Union Ave => Union Avenue\n",
      "10th Ave => 10th Avenue\n",
      "Third Ave => Third Avenue\n",
      "Manhattan Avenue (2nd Floor) => Manhattan Avenue (2nd Floor)\n",
      "Old Fulton => Old Fulton\n",
      "Track => Track\n",
      "Stuyvesant Oval => Stuyvesant Oval\n",
      "Highline => Highline\n",
      "Madison Avenue, 15th Floor => Madison Avenue, 15th Floor\n",
      "Wall Street 12th Floor => Wall Street 12th Floor\n",
      "519 9th Ave, New York, NY 10018, Estados Unidos => 519 9th Ave, New York, NY 10018, Estados Unidos\n",
      "3rd Ave, New York, NY 10028, Stati Uniti => 3rd Ave, New York, NY 10028, Stati Uniti\n",
      "Broadway. => Broadway.\n",
      "5th AVE 1807 => 5th AVE 1807\n",
      "East 80th Street, 21G => East 80th Street, 21G\n",
      "Madison Ave Arcage Level => Madison Avenue Arcage Level\n",
      "Lafayette => Lafayette\n",
      "Central Park South Suite 109 => Central Park South Suite 109\n",
      "E 55th St Ste. 301 => East 55th Street Ste. 301\n",
      "Hudson Yards => Hudson Yards\n",
      "36th St Front 1 => 36th Street Front 1\n",
      "334 Furman St, Brooklyn => 334 Furman St, Brooklyn\n",
      "110 SIXTH AVE. AT WATTS ST => 110 SIXTH AVE. AT WATTS Street\n",
      "N 9th ST => North 9th Street\n",
      "Vernon Blvd => Vernon Blvd\n",
      "Irvine Place, #1, New York, NY, 10003 => Irvine Place, #1, New York, NY, 10003\n",
      "Northside Piers => Northside Piers\n",
      "Chelsea Piers => Chelsea Piers\n",
      "633 => 633\n",
      "43rd Rd => 43rd Rd\n",
      "Grand Central Terminal => Grand Central Terminal\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Your task in this exercise has two steps:\n",
    "\n",
    "- audit the OSMFILE and change the variable 'mapping' to reflect the changes needed to fix \n",
    "    the unexpected street types to the appropriate ones in the expected list.\n",
    "    You have to add mappings only for the actual problems you find in this OSMFILE,\n",
    "    not a generalized solution, since that may and will depend on the particular area you are auditing.\n",
    "- write the update_name function, to actually fix the street name.\n",
    "    The function takes a string with street name as an argument and should return the fixed name\n",
    "    We have provided a simple test so that you see what exactly is expected\n",
    "\"\"\"\n",
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pprint\n",
    "\n",
    "OSMFILE = \"NYC.osm\"\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "st_types = defaultdict(set)\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\", \"Bend\", \"Chase\", \"Circle\", \"Cove\", \"Crossing\", \"Hill\",\n",
    "            \"Hollow\", \"Loop\", \"Park\", \"Pass\", \"Overlook\", \"Path\", \"Plaza\", \"Point\", \"Ridge\", \"Row\",\n",
    "            \"Run\", \"Terrace\", \"Walk\", \"Way\", \"Trace\", \"View\", \"Vista\",\"Concourse\",\"South\",\"North\",\"East\",\n",
    "             \"West\",\"Mews\",\"Broadway\",\"Alley\",\"street\",\"avenue\",\"Americas\",\"Village\",\"Bowery\"]\n",
    "             \n",
    "\n",
    "# UPDATE THIS VARIABLE\n",
    "mapping = { \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "            \"st\": \"Street\",\n",
    "            \"st.\": \"Street\",\n",
    "            \"ST\": \"Street\",\n",
    "            \"Ave\": \"Avenue\",\n",
    "            \"ave\": \"Avenue\",\n",
    "            \"Avene\": \"Avenue\",\n",
    "            \"avene\": \"Avenue\",\n",
    "            \"Aveneu\": \"Avenue\",\n",
    "             \"steet\": \"Street\",\n",
    "            \"Steet\": \"Street\",            \n",
    "            \"Rd.\": \"Road\",\n",
    "            \"W\": \"West\",\n",
    "            \"N\": \"North\",\n",
    "            \"S\": \"South\",\n",
    "            \"E\": \"East\"}\n",
    "\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"rb\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "\n",
    "    return street_types\n",
    "\n",
    "\n",
    "def update_name(name, mapping):\n",
    "    after = []\n",
    "    # Split name string to test each part of the name;\n",
    "    # Replacements may come anywhere in the name.\n",
    "    for part in name.split(\" \"):\n",
    "        # Check each part of the name against the keys in the correction dict\n",
    "        if part in mapping.keys():\n",
    "            # If exists in dict, overwrite that part of the name with the dict value for it.\n",
    "            part = mapping[part]\n",
    "        # Assemble each corrected piece of the name back together.\n",
    "        after.append(part)\n",
    "    # Return all pieces of the name as a string joined by a space.\n",
    "    return \" \".join(after)\n",
    "    \n",
    "\n",
    "#     for w in mapping.keys():\n",
    "#         if w in name:\n",
    "#             if flag:\n",
    "#                 continue\n",
    "#             # Replace abbrev. name in string with full name value from the mapping dict.\n",
    "#             name = name.replace(w, mapping[w], 1)\n",
    "#             # If St., flag to not check again in this string looking for St since new 'Street' will contain St\n",
    "#             # re.compile() might be better\n",
    "#             if w == \"St.\":\n",
    "#                 flag = True\n",
    "\n",
    "    return name\n",
    "\n",
    "\n",
    "def test():\n",
    "    st_types = audit(OSMFILE)\n",
    "    #assert len(st_types) == 3\n",
    "    pprint.pprint(dict(st_types))\n",
    "\n",
    "    for st_type, ways in st_types.items():\n",
    "        for name in ways:\n",
    "            better_name = update_name(name, mapping)\n",
    "            print (name, \"=>\", better_name)\n",
    "           \n",
    "\n",
    "\n",
    "\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Postal Codes \n",
    "\n",
    "Another problem that I noticed while parsing through the data was that the postal codes were presented in different ways. Below is an excerpt of some of the different formats for postal codes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible problematic zip codes:\n",
      "\n",
      "\n",
      "Fixed Zip:    10020-2402 => 10020\n",
      "Fixed Zip:    NY 11201 => 11201\n",
      "Fixed Zip:    10011-6832 => 10011\n",
      "Fixed Zip:    NY 10012 => 10012\n",
      "Fixed Zip:    NY 10002 => 10002\n",
      "Fixed Zip:    NY 10111 => 10111\n",
      "Fixed Zip:    10012-3332 => 10012\n",
      "Fixed Zip:    NY 10075 => 10075\n",
      "Fixed Zip:    10018-4527 => 10018\n",
      "Fixed Zip:    NY 10075 => 10075\n",
      "Fixed Zip:    10016-0122 => 10016\n",
      "Fixed Zip:    NY 10036 => 10036\n",
      "Fixed Zip:    NY 10011 => 10011\n",
      "Fixed Zip:    100014 => 10001\n",
      "Fixed Zip:    NY 10003 => 10003\n",
      "Fixed Zip:    NY 10003 => 10003\n",
      "Fixed Zip:    10017-6927 => 10017\n",
      "Fixed Zip:    NY 11201 => 11201\n",
      "Fixed Zip:    10075-0381 => 10075\n",
      "Fixed Zip:    NY 10003 => 10003\n",
      "Fixed Zip:    NY 10036 => 10036\n",
      "Fixed Zip:    NY 10036 => 10036\n",
      "Fixed Zip:    NY 10036 => 10036\n",
      "Fixed Zip:    NY 10007 => 10007\n",
      "Fixed Zip:    NY  10011 => 10011\n",
      "Fixed Zip:    NY 10011 => 10011\n",
      "Fixed Zip:    NY 10001 => 10001\n",
      "Fixed Zip:    10001-2062 => 10001\n",
      "Fixed Zip:    10019-9998 => 10019\n",
      "Fixed Zip:    NY 10016 => 10016\n",
      "Fixed Zip:    New York, NY 10065 => 10065\n",
      "Fixed Zip:    10002-1013 => 10002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'07086',\n",
       " '10001',\n",
       " '10002',\n",
       " '10003',\n",
       " '10004',\n",
       " '10005',\n",
       " '10006',\n",
       " '10007',\n",
       " '10009',\n",
       " '10010',\n",
       " '10011',\n",
       " '10012',\n",
       " '10013',\n",
       " '10014',\n",
       " '10016',\n",
       " '10017',\n",
       " '10018',\n",
       " '10019',\n",
       " '10020',\n",
       " '10021',\n",
       " '10022',\n",
       " '10023',\n",
       " '10028',\n",
       " '10036',\n",
       " '10038',\n",
       " '10044',\n",
       " '10045',\n",
       " '10048',\n",
       " '10055',\n",
       " '10065',\n",
       " '10069',\n",
       " '10075',\n",
       " '10103',\n",
       " '10107',\n",
       " '10110',\n",
       " '10111',\n",
       " '10112',\n",
       " '10118',\n",
       " '10121',\n",
       " '10123',\n",
       " '10128',\n",
       " '10152',\n",
       " '10153',\n",
       " '10154',\n",
       " '10155',\n",
       " '10168',\n",
       " '10169',\n",
       " '10173',\n",
       " '10174',\n",
       " '10271',\n",
       " '10275',\n",
       " '10280',\n",
       " '10281',\n",
       " '10282',\n",
       " '11101',\n",
       " '11106',\n",
       " '11109',\n",
       " '11201',\n",
       " '11206',\n",
       " '11211',\n",
       " '11222',\n",
       " '11226',\n",
       " '11249',\n",
       " '11251'}"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_zip_code(elem):\n",
    "    return (elem.attrib['k'] == \"addr:postcode\")\n",
    "\n",
    "def audit_zip(osmfile):\n",
    "    osm_file = open(osmfile, \"rb\")\n",
    "    prob_zip = set()\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_zip_code(tag):\n",
    "\n",
    "                    if len(tag.attrib['v']) != 5:\n",
    "                        if (tag.attrib['v'][0])=='N':\n",
    "                            print (\"Fixed Zip:   \", tag.attrib['v'], \"=>\", tag.attrib['v'][-5:])\n",
    "                        else:\n",
    "                            print (\"Fixed Zip:   \", tag.attrib['v'], \"=>\", tag.attrib['v'][0:5])                        \n",
    "                                              \n",
    "                    elif tag.attrib['v'][0:2] != '96':\n",
    "                        #print (\"Fixed Zip:   \", tag.attrib['v'], \"=>\", tag.attrib['v'][0:5])\n",
    "                        prob_zip.add(tag.attrib['v'])\n",
    "                    elif len(tag.attrib['v']) == 5:\n",
    "                        prob_zip.add(tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return prob_zip\n",
    "\n",
    "print (\"Possible problematic zip codes:\")\n",
    "audit_zip('NYC.osm')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Functions to update  and correct the  abbreviated street names and postal codes before uploading into csv files to load into the sqllite database__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_element(elem):\n",
    "    \n",
    "    # Fix Street Names:\n",
    "\n",
    "    # mapping provides a dictionary for updating potentially problematic street type names.\n",
    "    # Dictionary contents were updated iteratively, based on the audit results\n",
    "    # UPDATE THIS VARIABLE\n",
    "    \n",
    "    mapping = { \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "            \"st\": \"Street\",\n",
    "            \"st.\": \"Street\",\n",
    "            \"ST\": \"Street\",\n",
    "            \"Ave\": \"Avenue\",\n",
    "            \"ave\": \"Avenue\",\n",
    "            \"Avene\": \"Avenue\",\n",
    "            \"avene\": \"Avenue\",\n",
    "            \"Aveneu\": \"Avenue\",\n",
    "             \"steet\": \"Street\",\n",
    "            \"Steet\": \"Street\",            \n",
    "            \"Rd.\": \"Road\",\n",
    "            \"W\": \"West\",\n",
    "            \"N\": \"North\",\n",
    "            \"S\": \"South\",\n",
    "            \"E\": \"East\"}\n",
    "\n",
    "def fix_street(elem):\n",
    "    street_types = defaultdict(set)\n",
    "    if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "        for tag in elem.iter(\"tag\"):\n",
    "            if is_street_name(tag):\n",
    "                audit_street_type(street_types, tag.attrib['v'])\n",
    "                for st_type, ways in street_types.iteritems():\n",
    "                    for name in ways:\n",
    "                        for key,value in mapping.items():\n",
    "                            n = street_type_re.search(name)\n",
    "                            if n:\n",
    "                                street_type = n.group()\n",
    "                                if street_type not in expected:\n",
    "                                    if street_type in mapping:\n",
    "                                        better_name = name.replace(key,value)\n",
    "                                        if better_name != name:\n",
    "                                            print (\"Fixed Street:\", tag.attrib['v'], \"=>\", better_name)\n",
    "                                            tag.attrib['v'] = better_name\n",
    "                                            return\n",
    "    # Fix Zip Codes:\n",
    "\n",
    "def fix_zip(elem):\n",
    "    if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "        for tag in elem.iter(\"tag\"):\n",
    "            if is_zip_code(tag):\n",
    "                if len(tag.attrib['v']) != 5:\n",
    "                    if (tag.attrib['v'][0])=='N':\n",
    "                        #print (\"Fixed Zip:   \", tag.attrib['v'], \"=>\", tag.attrib['v'][-5:])\n",
    "                        tag.attrib['v'] = tag.attrib['v'][-5:]\n",
    "                    else:\n",
    "                        #print (\"Fixed Zip:   \", tag.attrib['v'], \"=>\", tag.attrib['v'][0:5])\n",
    "                        tag.attrib['v'] = tag.attrib['v'][0:5]\n",
    "                            \n",
    "                            \n",
    "\n",
    "    fix_street(elem)\n",
    "    fix_zip(elem)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Importing Dataset to Database \n",
    "\n",
    "After performing the most of the cleaning through Python, I can store the dataset in the database to examine the PROBLEMATIC elements and explore it further.\n",
    " I am using sqllite to present a generic solution . Initially, I am exporting the data in .csv files using the schema below, creating the tables in sqllite database  and importing the .csvs.\n",
    "\n",
    "### Exporting dataset to .CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SCHEMA = {\n",
    "    'node': {\n",
    "        'type': 'dict',\n",
    "        'schema': {\n",
    "            'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'lat': {'required': True, 'type': 'float', 'coerce': float},\n",
    "            'lon': {'required': True, 'type': 'float', 'coerce': float},\n",
    "            'user': {'required': True, 'type': 'string'},\n",
    "            'uid': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'version': {'required': True, 'type': 'string'},\n",
    "            'changeset': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'timestamp': {'required': True, 'type': 'string'}\n",
    "        }\n",
    "    },\n",
    "    'node_tags': {\n",
    "        'type': 'list',\n",
    "        'schema': {\n",
    "            'type': 'dict',\n",
    "            'schema': {\n",
    "                'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'key': {'required': True, 'type': 'string'},\n",
    "                'value': {'required': True, 'type': 'string'},\n",
    "                'type': {'required': True, 'type': 'string'}\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'way': {\n",
    "        'type': 'dict',\n",
    "        'schema': {\n",
    "            'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'user': {'required': True, 'type': 'string'},\n",
    "            'uid': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'version': {'required': True, 'type': 'string'},\n",
    "            'changeset': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'timestamp': {'required': True, 'type': 'string'}\n",
    "        }\n",
    "    },\n",
    "    'way_nodes': {\n",
    "        'type': 'list',\n",
    "        'schema': {\n",
    "            'type': 'dict',\n",
    "            'schema': {\n",
    "                'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'node_id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'position': {'required': True, 'type': 'integer', 'coerce': int}\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'way_tags': {\n",
    "        'type': 'list',\n",
    "        'schema': {\n",
    "            'type': 'dict',\n",
    "            'schema': {\n",
    "                'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'key': {'required': True, 'type': 'string'},\n",
    "                'value': {'required': True, 'type': 'string'},\n",
    "                'type': {'required': True, 'type': 'string'}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the files\n",
    "\n",
    "OSM_PATH = \"example.osm\"\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\, \\t\\r\\n]')\n",
    "# Make sure the fields order in the csvs matches the column order \n",
    "#in the sql table schema\n",
    "NODE_FIELDS = ['id', 'lat', 'lon','user','uid','version','changeset','timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shape_element(element):\n",
    "    \"\"\"Clean and shape node or way XML element to Python dict\n",
    "    \n",
    "    Arrgs:\n",
    "        element (element): An element of the XML tree\n",
    "        \n",
    "    Returns:\n",
    "        dict: if element is a node, the node's attributes and tags.\n",
    "              if element is a way, the ways attributes and tags along with the \n",
    "              nodes that form the way.\n",
    "    \"\"\"\n",
    "    node_attribs = {}\n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []  # Handle secondary tags the same way for both node and way elements\n",
    "    fix_element(element)\n",
    "    if element.tag == 'node':\n",
    "        node_attribs['id'] = element.get('id')\n",
    "        node_attribs['lat'] = element.get('lat')\n",
    "        node_attribs['lon'] = element.get('lon')\n",
    "        node_attribs['user'] = element.get('user')\n",
    "        node_attribs['uid'] = element.get('uid')\n",
    "        node_attribs['version'] = element.get('version')\n",
    "        node_attribs['changeset'] = element.get('changeset')\n",
    "        node_attribs['timestamp'] = element.get('timestamp')\n",
    "        for child in element:\n",
    "            if child.tag == 'tag':\n",
    "                tag = {'id': node_attribs['id']}\n",
    "                k = child.get('k')\n",
    "                if not PROBLEMCHARS.search(k):\n",
    "                    k = k.split(':', 1)\n",
    "                    tag['key'] = k[-1]\n",
    "                    tag['value'] = child.get('v')\n",
    "                    if len(k) == 1:\n",
    "                        tag['type'] = 'regular'\n",
    "                    elif len(k) == 2:\n",
    "                        tag['type'] = k[0]\n",
    "                tags.append(tag)                         \n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "    elif element.tag == 'way':\n",
    "        counter = 0\n",
    "        way_attribs['id'] = element.get('id')\n",
    "        way_attribs['user'] = element.get('user')\n",
    "        way_attribs['uid'] = element.get('uid')\n",
    "        way_attribs['version'] = element.get('version')\n",
    "        way_attribs['changeset'] = element.get('changeset')\n",
    "        way_attribs['timestamp'] = element.get('timestamp')\n",
    "        for child in element:\n",
    "            if child.tag == 'tag':\n",
    "                tag = {'id': way_attribs['id']}\n",
    "                k = child.get('k')\n",
    "                if not PROBLEMCHARS.search(k):\n",
    "                    k = k.split(':', 1)\n",
    "                    tag['key'] = k[-1]\n",
    "                    tag['value'] = child.get('v')\n",
    "                    if len(k) == 1:\n",
    "                        tag['type'] = 'regular'\n",
    "                    elif len(k) == 2:\n",
    "                        tag['type'] = k[0]\n",
    "                tags.append(tag)\n",
    "            if child.tag == 'nd':\n",
    "                nd = {'id': way_attribs['id']}\n",
    "                nd['node_id'] = child.get('ref')\n",
    "                nd['position'] = counter\n",
    "                way_nodes.append(nd)\n",
    "            counter += 1\n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate_element(element, validator, schema=SCHEMA):\n",
    "    \"\"\"Raise ValidationError if element does not match schema\"\"\"\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(iter(validator.errors.items()))\n",
    "        pprint.pprint(element)\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_string = pprint.pformat(errors)\n",
    "        \n",
    "        raise Exception(message_string.format(field, error_string))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, unicode) else v)\n",
    "            for k, v in row.items()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_map(file_in, validate):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "\n",
    "    with codecs.open(NODES_PATH, 'w') as nodes_file, \\\n",
    "         codecs.open(NODE_TAGS_PATH, 'w') as nodes_tags_file, \\\n",
    "         codecs.open(WAYS_PATH, 'w') as ways_file, \\\n",
    "         codecs.open(WAY_NODES_PATH, 'w') as way_nodes_file, \\\n",
    "         codecs.open(WAY_TAGS_PATH, 'w') as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "        validator = cerberus.Validator()\n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            #pprint.pprint(element)\n",
    "            el = shape_element(element)\n",
    "            #pprint.pprint(el)\n",
    "            if el:\n",
    "                if validate is True:\n",
    "                    validate_element(el, validator)\n",
    "\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "process_map('NYC.osm', validate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The csv files generated are imported into the sqllite database names nycosmdata.db\n",
    "Now let us explore our data more using SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>user</th>\n",
       "      <th>uid</th>\n",
       "      <th>version</th>\n",
       "      <th>changeset</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30978735</td>\n",
       "      <td>40.773892</td>\n",
       "      <td>-73.971605</td>\n",
       "      <td>mikercpc</td>\n",
       "      <td>4124310</td>\n",
       "      <td>4</td>\n",
       "      <td>40180450</td>\n",
       "      <td>2016-06-21T13:06:21Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30978738</td>\n",
       "      <td>40.773908</td>\n",
       "      <td>-73.972081</td>\n",
       "      <td>mikercpc</td>\n",
       "      <td>4124310</td>\n",
       "      <td>4</td>\n",
       "      <td>40180450</td>\n",
       "      <td>2016-06-21T13:06:21Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30978740</td>\n",
       "      <td>40.773899</td>\n",
       "      <td>-73.972518</td>\n",
       "      <td>mikercpc</td>\n",
       "      <td>4124310</td>\n",
       "      <td>5</td>\n",
       "      <td>40180450</td>\n",
       "      <td>2016-06-21T13:06:21Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30978741</td>\n",
       "      <td>40.773899</td>\n",
       "      <td>-73.972834</td>\n",
       "      <td>mikercpc</td>\n",
       "      <td>4124310</td>\n",
       "      <td>4</td>\n",
       "      <td>40180450</td>\n",
       "      <td>2016-06-21T13:06:21Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30978743</td>\n",
       "      <td>40.773990</td>\n",
       "      <td>-73.973139</td>\n",
       "      <td>mikercpc</td>\n",
       "      <td>4124310</td>\n",
       "      <td>4</td>\n",
       "      <td>40180450</td>\n",
       "      <td>2016-06-21T13:06:21Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id        lat        lon      user      uid  version  changeset  \\\n",
       "0  30978735  40.773892 -73.971605  mikercpc  4124310        4   40180450   \n",
       "1  30978738  40.773908 -73.972081  mikercpc  4124310        4   40180450   \n",
       "2  30978740  40.773899 -73.972518  mikercpc  4124310        5   40180450   \n",
       "3  30978741  40.773899 -73.972834  mikercpc  4124310        4   40180450   \n",
       "4  30978743  40.773990 -73.973139  mikercpc  4124310        4   40180450   \n",
       "\n",
       "              timestamp  \n",
       "0  2016-06-21T13:06:21Z  \n",
       "1  2016-06-21T13:06:21Z  \n",
       "2  2016-06-21T13:06:21Z  \n",
       "3  2016-06-21T13:06:21Z  \n",
       "4  2016-06-21T13:06:21Z  "
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import SQLLITE to explore the data loaded from csv files into nycosmdata.db\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "conn = sqlite3.connect(\"nycosmdata.db\")\n",
    "df = pd.read_sql_query(\"select * from nodes limit 5;\", conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('146130   nodes_tags',),\n",
      " ('408564   ways_tags',),\n",
      " ('412798  nodes',),\n",
      " ('618229   ways_nodes',),\n",
      " ('71682   ways',)]\n"
     ]
    }
   ],
   "source": [
    "#Letus check the number of rows in each table \n",
    "\n",
    "conn = sqlite3.connect(\"nycosmdata.db\")\n",
    "cur = conn.cursor()\n",
    "counts = cur.execute(\"\"\"select count(*)||'  nodes' from nodes union  \n",
    "                        Select count(*)||'   nodes_tags' from nodes_tags union\n",
    "                        Select count(*)||'   ways' from ways union\n",
    "                        Select count(*)||'   ways_nodes' from ways_nodes union\n",
    "                        Select count(*)||'   ways_tags' from ways_tags\n",
    "    ;\"\"\").fetchall()\n",
    "#results = cur.fetchall()\n",
    "pprint.pprint(counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1756,)]\n"
     ]
    }
   ],
   "source": [
    "# Number of unique users:\n",
    "\n",
    "no_of_users = cur.execute(\"\"\"SELECT COUNT(DISTINCT(users.uid))          \n",
    "FROM (SELECT uid FROM nodes UNION ALL SELECT uid FROM ways) users;\"\"\").fetchall()\n",
    "print(no_of_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Rub21_nycbuildings', 217134),\n",
      " ('lxbarth_nycbuildings', 71329),\n",
      " ('robge', 63708),\n",
      " ('ALE!', 13715),\n",
      " ('mikercpc', 10699),\n",
      " ('minewman', 8758),\n",
      " ('Korzun', 6626),\n",
      " ('tre1994', 4015),\n",
      " ('celosia_nycbuildings', 3570),\n",
      " ('LizBarry_nycbuildings', 2869)]\n"
     ]
    }
   ],
   "source": [
    "# Top 10 users\n",
    "\n",
    "Top_10_users = cur.execute(\"\"\"SELECT nodes_ways.\"user\" AS \"User\", COUNT(*) AS \"Users\"\n",
    "FROM (SELECT \"user\" FROM nodes\n",
    "      UNION ALL\n",
    "      SELECT \"user\" FROM ways) AS nodes_ways\n",
    "GROUP BY nodes_ways.\"user\"\n",
    "ORDER BY \"Users\" DESC\n",
    "LIMIT 10;\"\"\").fetchall()\n",
    "pprint.pprint(Top_10_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('11211', 6148),\n",
      " ('11222', 5883),\n",
      " ('10011', 2826),\n",
      " ('10003', 2565),\n",
      " ('10014', 2551),\n",
      " ('10002', 2487),\n",
      " ('11206', 2300),\n",
      " ('11101', 2256),\n",
      " ('10013', 2189),\n",
      " ('11249', 1944)]\n"
     ]
    }
   ],
   "source": [
    "# Top 10 zip codes\n",
    "\n",
    "top_zip_codes = cur.execute(\"\"\"SELECT tags.value, COUNT(*) as count \n",
    "FROM (SELECT * FROM nodes_tags \n",
    "      UNION ALL \n",
    "      SELECT * FROM ways_tags) tags\n",
    "WHERE tags.key='postcode'\n",
    "GROUP BY tags.value\n",
    "ORDER BY count DESC\n",
    "LIMIT 10;\"\"\").fetchall()\n",
    "pprint.pprint(top_zip_codes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bicycle_parking', 5396),\n",
      " ('restaurant', 3256),\n",
      " ('cafe', 1166),\n",
      " ('fast_food', 788),\n",
      " ('bicycle_rental', 668),\n",
      " ('bar', 636),\n",
      " ('bank', 490),\n",
      " ('bench', 380),\n",
      " ('school', 380),\n",
      " ('embassy', 326)]\n"
     ]
    }
   ],
   "source": [
    "# Top amenities in NYC\n",
    "\n",
    "top_amenities = cur.execute(\"\"\"SELECT value AS \"Amenity\", \n",
    "COUNT(value) AS \"Occurrences\"\n",
    "FROM(SELECT *\n",
    "FROM nodes_tags\n",
    "UNION ALL\n",
    "SELECT *\n",
    "FROM nodes_tags) as tags\n",
    "WHERE key = 'amenity'\n",
    "GROUP BY value\n",
    "ORDER BY \"Occurrences\" DESC\n",
    "LIMIT 10\"\"\").fetchall()\n",
    "pprint.pprint(top_amenities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is intresting to note that bicycle parking is the top amenity in Newyork city. I was expecting restaurants to comeup first but this makes sense when you think about the really high amount of bicycle usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Chase', 77),\n",
      " ('Citibank', 44),\n",
      " ('Bank of America', 36),\n",
      " ('TD Bank', 26),\n",
      " ('HSBC', 23),\n",
      " ('Capital One', 20),\n",
      " ('Wells Fargo', 10),\n",
      " ('Santander', 9),\n",
      " ('Valley National Bank', 8),\n",
      " ('UNFCU', 6)]\n"
     ]
    }
   ],
   "source": [
    "top_banks = cur.execute(\"\"\" SELECT value AS \"Bank\", COUNT(value) AS \"ATMs\"\n",
    "FROM (Select * from nodes_tags union select * from ways_tags)\n",
    "       WHERE id in(SELECT id from (Select * from nodes_tags \n",
    "                   union select * from ways_tags)\n",
    "                    WHERE upper(value) = 'ATM' or upper(value) = 'BANK')\n",
    "       AND  upper(key) in ('NAME','OPERATOR')\n",
    "       GROUP BY value ORDER BY \"ATMs\" DESC LIMIT 10;\"\"\").fetchall()\n",
    "pprint.pprint(top_banks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cafe', 606), ('hotel', 379), ('restaurant', 1728)]\n"
     ]
    }
   ],
   "source": [
    "#Number of cafes, hotels, pubs, and restaurants:\n",
    "    \n",
    "top_restaurants = cur.execute(\"\"\"SELECT value, COUNT(*)\n",
    "FROM (SELECT * from nodes_tags as T UNION ALL \n",
    "      SELECT * from ways_tags as Z) as Q\n",
    "WHERE (value = 'restaurant' OR value = 'hotel' OR  \n",
    "       value = 'pub' OR value = 'cafe')\n",
    "GROUP BY value \"\"\").fetchall()\n",
    "pprint.pprint(top_restaurants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('coffee_shop', 162),\n",
      " ('italian', 117),\n",
      " ('pizza', 109),\n",
      " ('mexican', 96),\n",
      " ('burger', 87),\n",
      " ('american', 83),\n",
      " ('chinese', 56),\n",
      " ('japanese', 55),\n",
      " ('sandwich', 46),\n",
      " ('indian', 44)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "top_cusines = cur.execute(\"\"\"SELECT value, COUNT(*)\n",
    "FROM (SELECT * from nodes_tags as T UNION ALL \n",
    "      SELECT * from ways_tags as Z) as Q\n",
    "WHERE (key = 'cuisine') group by value ORDER BY 2 DESC LIMIT 10;\"\"\").fetchall()\n",
    "pprint.pprint(top_cusines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the number of restaurants by cusine tells me they are way under reported and on further analysis the values reported were vastly different .Hence they weren't grouped together. This also gave me ideas for how the data might be improved in the future, which I discuss in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Ideas \n",
    "\n",
    "There are several areas of improvement of the project in the future. The first one is on the completeness of the data. All the above analysis is based on a dataset that reflects a big part of Newyork city manhattab but not the whole island. The reason for this is the lack of a way to download a dataset for the entire place without including parts of the neighboring areas. The analyst has to either select a part of the city or select a wider area that includes parts of other NYC boroughs and parts of New jersey. \n",
    "\n",
    "The data could be improved by standardizing the information that is included with the node tags for place like cafe, hotel, pub, and restaurant upon data entry. Some tags give a great deal of information about the establishment while others provide very little detail. This would help to increase the quality of the openstreetmap data and enhance the consumer experience. The financial institutions are not listed with uniformity either.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "https://download.geofabrik.de/north-america.html\n",
    "https://wiki.openstreetmap.org/wiki/OSM_XML\n",
    "https://gist.github.com/carlward/54ec1c91b62a5f911c42#file-sample_project-md\n",
    "https://github.com/YannisPap/Wrangle-OpenStreetMap-Data/blob/master/Notebook/Wrangle-OpenStreetMap-Data.ipynb\n",
    "http://overpass-api.de/\n",
    "https://www.python.org/\n",
    "       \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
